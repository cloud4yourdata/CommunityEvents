{
	"name": "DeltaMerge",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "demo",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "1"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/ab86deda-c257-42d0-bd53-04612d5ba4eb/resourceGroups/sqlday2020demos/providers/Microsoft.Synapse/workspaces/synapasedemos/bigDataPools/demo",
				"name": "demo",
				"type": "Spark",
				"endpoint": "https://synapasedemos.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/demo",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "2.4",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"users_df1 = spark.read.parquet('abfss://datalake@datalake2demos.dfs.core.windows.net/rawdata/UserDB/users/yyyyMMdd=20201120')\r\n",
					"users_df1.show()"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"from pyspark.sql.functions import sha2, concat_ws, lit, col,current_timestamp,unix_timestamp,date_add\r\n",
					"hash_column_list = 'UserName,UserRole'\r\n",
					"\r\n",
					"cols = set(hash_column_list.split(','))\r\n",
					"df = users_df1.withColumn('dwh_pk',lit(\"0\")).withColumn(\"dwh_row_sha2\", sha2(concat_ws(\"||\", *cols), 256)) \\\r\n",
					"    .withColumn(\"row_datetime\",lit(current_timestamp())) \\\r\n",
					"    .withColumn(\"row_iscurrent\",lit(True))\\\r\n",
					"    .withColumn(\"row_startdate\",lit('2000-01-01').cast('timestamp')) \\\r\n",
					"    .withColumn(\"row_enddate\",lit('9999-12-31').cast('timestamp')) \r\n",
					"df.show()"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"from delta.tables import *\r\n",
					"delta_path='abfss://datalake@datalake2demos.dfs.core.windows.net/analytics_zone/SynapseDWH/'\r\n",
					"delta_table_path=f'{delta_path}/users'\r\n",
					"if not DeltaTable.isDeltaTable(spark,delta_table_path):\r\n",
					"    df.createOrReplaceTempView('tmp_to_load_users')\r\n",
					"    #PARTITIONED BY\r\n",
					"    sql=f\"CREATE TABLE users USING DELTA LOCATION '{delta_table_path}' AS (SELECT * FROM tmp_to_load_users)\"\r\n",
					"    spark.sql(sql)"
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"dwh_data = DeltaTable.forPath(spark,delta_table_path)\r\n",
					"dwh_data.generate(\"symlink_format_manifest\")"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"users_df2 = spark.read.parquet('abfss://datalake@datalake2demos.dfs.core.windows.net/rawdata/UserDB/users/yyyyMMdd=20201121')\r\n",
					"users_df2.show()"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}